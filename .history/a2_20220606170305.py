# -*- coding: utf-8 -*-
"""A2 RE10-470534496-Jeff-Luo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BmsW8DVF6TLoB8XDzwqCs1_7evfJgg8k

# Introduction

This notebook is used for Data Preproceessing and Performance Tuning with Spark as required by COMP5349 assignment 2 implementation
"""

# !pip install pyspark

"""# Data Loading"""

import argparse

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.functions import explode, broadcast, explode_outer, col, udf
from pyspark.sql.functions import when
from pyspark.sql.types import *
from pyspark.sql.window import Window

# config the spark session
# spark = SparkSession.builder.appName("A2").getOrCreate()
spark = SparkSession \
    .builder \
    .appName("A2") \
    .config("spark.sql.shuffle.partitions", 10) \
    .config("spark.executor.cores", "4") \
    .config("spark.executor.memory", "2G") \
    .config("spark.default.parallelism", 100) \
    .config("spark.eventLog.logBlockUpdates.enabled", True) \
    .config("spark.default.parallelism", 100) \
    .getOrCreate()

# spark.sparkContext.setLogLevel("ERROR")

parser = argparse.ArgumentParser()
parser.add_argument("--output", help="the output directory", default='out.json')
parser.add_argument("--dataset", help="dataset used", default='test')
args = parser.parse_args()

# file_dict = {'train': 's3://comp5349-2022/train_separate_questions.json',
#              'test': 's3://comp5349-2022/test.json',
#              'large': 's3://comp5349-2022/CUADv1.json'}

dataset_used = args.dataset
dataset_used = "test.json"

# read in data
raw_df = spark.read.json(file_dict.get(dataset_used))
raw_df = spark.read.json(dataset_used)
print("===== Check Schema of raw dataframe =====\n")
raw_df.printSchema()

"""# Convert JSON to desired Dataframe via SQL functions"""

# Explode to flatten values in 'data' column
# Then retrieve values in 'title' and 'paragraphs' column
# Then explode to flatten 'paragraphs' column
# Then select title, and 'context', 'qas' in the paragraphs columns
# Rename the columns as ["title", "context","qas"]
raw_sequences_df = raw_df.select((explode("data").alias('data'))) \
    .select("data.title", "data.paragraphs") \
    .select("title", explode("paragraphs")).toDF("title", "paragraphs") \
    .select("title", "paragraphs.context", "paragraphs.qas") \
    .toDF("title", "context", "qas").cache()
print("===== 5 rows of raw_sequences_df as (title, context, qas) =====\n")
raw_sequences_df.show(5)

# Keep original contract context, expand qas column, so we have question-answers per context
# Select the inner columns to extract the relevlant information in the qas column
paragraphs_per_context_df = raw_sequences_df.select("title", "context", explode("qas")).toDF("title", "context", "qas") \
    .select("title", "context", "qas.id", "qas.question", "qas.is_impossible", "qas.answers.answer_start",
            "qas.answers.text") \
    .toDF("title", "context", "id", "question", "is_impossible", "ans_start", "text")
print("===== 5 rows of paragraphs_per_context_df with relevant information extracted =====\n")
paragraphs_per_context_df.show(5)

"""## Segment each contract context"""


# Apply user defined function function to get segmented sequences into (start, end, source_text) formats
@udf(returnType=ArrayType(
    StructType([
        StructField('seq_start', IntegerType(), nullable=False),
        StructField('seq_end', IntegerType(), nullable=False),
        StructField("seq_source_text", StringType(), nullable=True)
    ])))
def segment(context):
    """Segment the context string in contract into sequences of 4096 characters except the last sequence"""
    WINDOW_SIZE = 4096
    STRIDE_SIZE = 2048
    start = 0
    res = []
    context_length = len(context)
    while start < context_length:
        end = start + WINDOW_SIZE
        if start + WINDOW_SIZE > context_length:
            end = context_length
        res.append((start, end, context[start:end]))
        start += STRIDE_SIZE
    return res


# Keep original qas in contract, segment the context column and explode to flatten it,
# so we have segmented sequences such that
# rows of [title, paragraphs] are converted into rows of [title, context_sequences, qas]
segmented_seq_df = raw_sequences_df.select("title", "context", explode(segment(col("context"))), "qas") \
    .toDF("title", "context", "context_seq", "qas")
print("===== 5 rows of (title, segmented_sequences_of_context, qas) =====\n")
segmented_seq_df.show(5)

# Explode 'qas' column to get rows of (title, context_sequences, qas)
# Then Apply explode_outer instead of explode on answers to include empty answers
# Rename columns
# Then further extract answers column to get answer_start and text columns
# Finally rename the columns to get sequences_df
# as [("title", "context_seq", "id", "question", "is_impossible", "ans_start", "text")]
sequences_df = segmented_seq_df.select("title", "context_seq", explode("qas")).toDF("title", "context_seq", "qas") \
    .select("title", "context_seq", "qas.id", "qas.is_impossible", "qas.question", explode_outer("qas.answers")) \
    .toDF("title", "context_seq", "id", "is_impossible", "question", "answers") \
    .select("title", "context_seq", "id", "question", "is_impossible", "answers.answer_start", "answers.text") \
    .toDF("title", "context_seq", "id", "question", "is_impossible", "ans_start", "text")

print('===== 5 rows of ("title", "context_seq", "id", "question", "is_impossible", "ans_start", "text") =====\n')
sequences_df.show(5)

"""## Add sequence type"""


# user defiend function to label the sequence with a type, as
# impossible_negative, possible_negative, or positive sample
@udf(returnType=StructType([
    StructField('seq_ans_start', IntegerType(), False),
    StructField('seq_ans_end', IntegerType(), False),
    StructField("seq_type", StringType(), True)
]))
def get_type(seq_context, is_impossible, answer_start, answer_text):
    """
    :param is_impossible: a boolean to represent if an answer is present
    :param seq_context: (source_text, seq_start, seq_end)
    :param answer_start: a number
    :param answer_text: a string
    :return: Type of the sample, a string
    """
    # impossible negative, zero occurrences of a particular category
    if is_impossible or not answer_text or not answer_start:
        return 0, 0, "impossible_N"

    seq_start, seq_end, seq_text = seq_context
    seq_length = seq_end - seq_start
    answer_end = answer_start + len(answer_text)
    # possible negative, is_impossibleâ€ is set to False, but sequence does not contain part of the answer.
    if seq_start > answer_end or seq_end < answer_start:
        return 0, 0, "possible_N"
    # positive sample that contains part or all of the answers to a question
    else:
        # compute the starting point of answer in seq, use 0 if seq_start > answer_start
        # get max
        seq_ans_start = answer_start - seq_start
        if answer_start - seq_start < 0:
            seq_ans_start = 0
        # compute the starting point of answer in seq, use shorter end as constrained by seq_end and ans_end
        # get min
        seq_ans_end = answer_end - seq_start
        if answer_end - seq_start > seq_length:
            seq_ans_end = seq_length
        return seq_ans_start, seq_ans_end, "P"


# Add type to the sequences_df
# Then expand the use_defined context_seq and type columns and selet all userful columns
#
print("Use get_type function to add impossible_negative, possible_negative, or positive sample type to the df\n")
print("20 rows of labelled_paragraphs with sequences indices and seq_type added:\n")
labelled_sequences_df = sequences_df.withColumn("type", \
                                                get_type(col("context_seq"), col("is_impossible"), col("ans_start"),
                                                         col("text"))) \
    .select("title", "text", "id", "question", \
            "context_seq.seq_start", "context_seq.seq_end", "context_seq.seq_source_text", \
            "type.seq_ans_start", "type.seq_ans_end", "type.seq_type")
labelled_sequences_df.show(20)

"""# Get positive samples for each ID"""

# Categorise and split the sample sequences by type positive
# Multiple actions on the same pos_df DataFrame are expected, so we cache it, ie, saves it to storage level 'MEMORY_AND_DISK'
X = labelled_sequences_df
pos_df = X.filter(X.seq_type == "P").cache()

# Select positive samples containing the desired columns
pos_res = pos_df.select("seq_source_text", "question", "seq_ans_start", "seq_ans_end") \
    .toDF("source", "question", "answer_start", "answer_end")

print("====== 20 Positive samples: =======")
pos_res.show(5)

"""# Compute count of impossible negative for each ID
As equal to average count of positive samples of each qn in other contracts
"""

# Filter the sample sequences by type impossible_negative or possible_negative
# Apply anti join to extract unique sequences of impossible negative
# Broadcast variables are used to save the copy of data across all nodes.
# This variable is cached on all the machines and not sent on machines with tasks.
impossible_neg_df = X.filter(X.seq_type == "impossible_N")
unique_impossible_neg_df = impossible_neg_df.join(broadcast(pos_df), ["seq_source_text"], "leftanti")
# print("====== 5 unique impossible negative samples: =======")
# unique_impossible_neg_df.show(5)

# Count number of positive samples, per qn
count_pos_per_qn = pos_df.groupBy("question").count().toDF("question", "count_pos_per_qn")
# Compute number of positive samples, per context per qn
count_pos_per_context_per_qn = pos_df.groupBy("id").count().toDF("id", "count_pos_per_context_per_qn")

# Join the above two result to pos_df, to add "count_pos_other_contracts_per_qn" to pos_df
count_pos_other_contracts_per_qn_full = pos_df.join(count_pos_per_context_per_qn, "id", "left") \
    .join(count_pos_per_qn, "question", "left") \
    .withColumn("count_pos_in_other_contracts_per_qn", col("count_pos_per_qn") - col("count_pos_per_context_per_qn")) \
    .select("id", "question", "count_pos_in_other_contracts_per_qn")
print("====== 10 counts of positive samples in other contracts, for each question: =======")
count_pos_other_contracts_per_qn_full.show(10)

# Count number of positive contracts, for each question
Y = paragraphs_per_context_df
count_contracts_per_qn = Y.filter(Y.is_impossible == False) \
    .groupBy("question").count().toDF("question", "count_contracts_per_qn")
# Count number of positive OTHER contracts, for each question
count_other_contracts_per_qn = count_contracts_per_qn.withColumn("count_other_contracts_per_qn", \
                                                                 count_contracts_per_qn.count_contracts_per_qn - 1)

print("====== 5 counts of other contracts, for each question: =======")
count_other_contracts_per_qn.show(5)
# Compute average number of positive samples of that question in other contracts
# as number of positive samples in other contracts / number of other contracts for each question
# Then we select ("title", "ave_pos_in_this_qn_other_contracts") to find for each question, its
# average number of positive samples in other contracts, 
ave_pos_column = when(col("count_other_contracts_per_qn") == 0, 0).otherwise(
    round((col("count_pos_in_other_contracts_per_qn") / col("count_other_contracts_per_qn"))))
ave_pos_this_qn_other_contracts = count_pos_other_contracts_per_qn_full.join(count_other_contracts_per_qn, "question",
                                                                             "left") \
    .withColumn("ave_pos_in_this_qn_other_contracts", ave_pos_column) \
    .select("question", "ave_pos_in_this_qn_other_contracts")
print("====== 5 counts for average number of pos samples in other contracts, for each question: =======")
ave_pos_this_qn_other_contracts.show(5)

# join the average number back to the impossible negative samples
# average number of positive samples of that question in other contracts
impossible_neg_df_short = unique_impossible_neg_df.select("id", "question", "seq_source_text", "question",
                                                          "seq_ans_start", "seq_ans_end")
impossible_neg_samples = unique_impossible_neg_df.join(broadcast(ave_pos_this_qn_other_contracts), "question")
# impossible_neg_samples.show(5)

# window function to rank the impossible negative samples for each contract
# use ave_pos_in_this_qn_other_contracts as criteria to only reserve the desired number of 
# the impossible negative samples for each question in each contract
partition_window = Window.partitionBy("id").orderBy("seq_ans_start")
imp_neg_res = impossible_neg_samples.withColumn("rank", row_number().over(partition_window)) \
    .filter(col("rank") <= col("ave_pos_in_this_qn_other_contracts")) \
    .select("seq_source_text", "question", "seq_ans_start", "seq_ans_end") \
    .toDF("source", "question", "answer_start", "answer_end")
print("====== 5 impossible negative samples as result: =======")
imp_neg_res.show(5)

"""# Compute count of Possible Negative = number of Positive samples for each ID"""

# Similarly, filter the sample sequences by type possible_negative, extract unique sequences of possible negative
possible_neg_df = X.filter(X.seq_type == "possible_N")
unique_possible_neg_df = possible_neg_df.join(broadcast(pos_df), ["seq_source_text"], "leftanti")
print("====== 5 possible negative samples: =======")
unique_possible_neg_df.show(5)

# join the positive sample number back to the possible negative samples
possible_neg_short = unique_possible_neg_df.select("id", "seq_source_text", "question", "seq_ans_start", "seq_ans_end")
possible_neg_samples = possible_neg_short.join(broadcast(count_pos_per_context_per_qn), "id")
# possible_neg_samples.show(5)

# Similarly only reserve the desired number of 
# the possible negative samples for each question in each contract
# as equal to the number of positive samples of that question in this contract.
pos_neg_res = possible_neg_samples \
    .withColumn("rank", row_number().over(Window.partitionBy("id").orderBy("seq_ans_start"))) \
    .filter(col("rank") <= col("count_pos_per_context_per_qn")) \
    .select("seq_source_text", "question", "seq_ans_start", "seq_ans_end") \
    .toDF("source", "question", "answer_start", "answer_end")
print("====== 5 possible negative samples as result: =======")
pos_neg_res.show(5)

# union all the samples
res = pos_res.union(imp_neg_res).union(pos_neg_res)

# res.show(50)
res.coalesce(1).write.json("output")
spark.stop()

"""END"""
